{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.50d.zip\n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.50d.zip  # to extract word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import os                                         # importing required libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocab(filename,d_model): # to generate word-num-vector mapping using pre trained word vectors\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    \n",
    "    num     = 0\n",
    "    vocab_num_to_vector = {}\n",
    "    vocab_word_to_num   = {}\n",
    "    vocab_num_to_vector[num] = np.zeros((d_model))\n",
    "    vocab_word_to_num['ukn'] = num\n",
    "    num+=1\n",
    "\n",
    "    for lines in file:\n",
    "        values = lines.split()\n",
    "        word   = values[0]\n",
    "        vector = list(map(float,values[1:]))\n",
    "        vocab_num_to_vector[num] = np.array(vector)\n",
    "        vocab_word_to_num[word]  = num\n",
    "        num+=1\n",
    "    \n",
    "    return vocab_word_to_num,vocab_num_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filenames,delimiter):   # extract data from all the files\n",
    "    \n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        \n",
    "        file = open(filename,'r')\n",
    "        \n",
    "        for lines in file:\n",
    "            values=lines.split(delimiter)\n",
    "            reviews.append(values[0])\n",
    "            ratings.append(int(values[-1]))\n",
    "\n",
    "    data = list(zip(reviews,ratings))\n",
    "    np.random.shuffle(data)\n",
    "    reviews,ratings=zip(*data)\n",
    "\n",
    "    reviews = np.array(reviews)\n",
    "    ratings = np.array(ratings).reshape(-1,1).astype('float32')\n",
    "    \n",
    "    return reviews,ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(word):   # removing non-alphabetic letters (punctuations and numbers)\n",
    "    \n",
    "    word       = list(word)\n",
    "    clean_word = []\n",
    "    \n",
    "    for i in range(len(word)):\n",
    "        \n",
    "        if ord(word[i])>=97 and ord(word[i])<=122:\n",
    "            clean_word.append(word[i])\n",
    "            \n",
    "    return ''.join(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanReviews(reviews,vocab_word_to_num):  # removing typoes and unknown words from reviews\n",
    "    \n",
    "    for i in range(len(reviews)):\n",
    "        review  = reviews[i]\n",
    "        words   = review.split()\n",
    "        clean_review = []\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = removePunctuations(word)\n",
    "            if word not in vocab_word_to_num.keys():\n",
    "                clean_review.append('ukn')\n",
    "            else:\n",
    "                clean_review.append(word)\n",
    "        \n",
    "        clean_review = ' '.join(clean_review)\n",
    "        reviews[i]   = clean_review\n",
    "        \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reviews,vocab_word_to_num,max_len): # tokenizing the review statements \n",
    "    \n",
    "    tokenized_reviews = np.zeros((len(reviews),max_len),dtype='int')\n",
    "    \n",
    "    for i in range(len(reviews)):\n",
    "        review = reviews[i]\n",
    "        review = review.split()\n",
    "        for j in range(len(review)):\n",
    "            tokenized_reviews[i][j] = vocab_word_to_num[review[j]]    \n",
    "        \n",
    "    return tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embd_matrix(vocab_num_to_vector,vocab_size,d_model):  # get the embedding matrix\n",
    "    \n",
    "    embd_matrix = np.zeros((vocab_size,d_model))\n",
    "    \n",
    "    for i in range(vocab_size):\n",
    "        embd_matrix[i] = vocab_num_to_vector[i]\n",
    "        \n",
    "    return embd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def(embd_matrix,vocab_size,d_model,max_len):  # define the model \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,d_model,weights=[embd_matrix],input_length=max_len,trainable=True,mask_zero=True))\n",
    "    model.add(LSTM(200,dropout=0.2,go_backwards=True))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    vocab_word_to_num,vocab_num_to_vector = getVocab('glove.6B.50d.txt',50)\n",
    "    reviews,ratings = getData(['amazon.txt','yelp.txt','imdb.txt'],'\\t')\n",
    "    reviews     = cleanReviews(reviews,vocab_word_to_num)\n",
    "    reviews     = tokenize(reviews,vocab_word_to_num,100)\n",
    "    embd_matrix = load_embd_matrix(vocab_num_to_vector,len(vocab_word_to_num),len(vocab_num_to_vector[0]))\n",
    "    model       = model_def(embd_matrix,len(vocab_word_to_num),50,100)\n",
    "    \n",
    "    train_length= 2700\n",
    "    x_train     = reviews[:train_length,:]\n",
    "    x_test      = reviews[train_length:,:]\n",
    "    y_train     = ratings[:train_length:,:]\n",
    "    y_test      = ratings[train_length:,:]\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 100, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 200)               200800    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,221,051\n",
      "Trainable params: 20,221,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "43/43 [==============================] - 18s 416ms/step - loss: 0.6241 - accuracy: 0.6527 - val_loss: 0.5534 - val_accuracy: 0.7480\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 17s 400ms/step - loss: 0.4935 - accuracy: 0.7760 - val_loss: 0.4361 - val_accuracy: 0.8080\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 17s 398ms/step - loss: 0.4241 - accuracy: 0.8113 - val_loss: 0.4009 - val_accuracy: 0.8280\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 17s 399ms/step - loss: 0.3710 - accuracy: 0.8451 - val_loss: 0.3774 - val_accuracy: 0.8400\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 18s 412ms/step - loss: 0.3307 - accuracy: 0.8593 - val_loss: 0.4119 - val_accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 18s 415ms/step - loss: 0.2923 - accuracy: 0.8873 - val_loss: 0.3555 - val_accuracy: 0.8480\n",
      "Epoch 7/10\n",
      "21/43 [=============>................] - ETA: 8s - loss: 0.2656 - accuracy: 0.8906"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
